---
title: "Pretty darn good control: when are approximate solutions better than approximate models"
titlerunning: Pretty darn good control
thanks: | 
    This material is based upon work supported by the National Science
    Foundation under Grant No. DBI-1942280. 

authors: 
- name: Felipe Montealegre-Mora
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Marcus Laperolerie
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Melissa Chapman
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Abigail G. Keller
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Carl Boettiger
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
  email: cboettig@berkeley.edu 


keywords:
- Optimal Control
- Reinforcement Learning
- Uncertainty
- Decision Theory

#PACS: 
#- PAC1
#- superPAC
    
MSC:
- MSC code 1
- MSC code 2

abstract: |
  Write something here
  
bibliography: bibliography.bib
#biblio-style: spphys

header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{lineno}
  - \usepackage{setspace}
  - \usepackage{bookmark}
  - \usepackage{float}
  - \usepackage{graphicx}
  - \usepackage{caption}
  - \usepackage{subcaption}
  - \usepackage{tikz-cd}
  - \linenumbers
  - \doublespacing

# bibstyle options spbasic(default), spphys, spmpsci
# output: rticles::springer_article

output: 
  rticles::arxiv_article:
    keep_tex: true

editor_options: 
  markdown: 
    wrap: sentence

---

```{r knit_global, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, 
                      warning = FALSE, fig.height=5, fig.width=7.5)
library(tidyverse)
library(patchwork)
```

```{r}
# for best const mortality and best const escapement
msy_optimizer <- function(df) {
  df |> 
  group_by(action, rep) |>
  filter(t == max(t)) |> 
  group_by(action) |>
  summarise(mean_reward = mean(reward), sd = sd(reward)) |> 
  filter(mean_reward == max(mean_reward))
}
escapement_optimizer <- function(df) {
  df |> 
  group_by(escapement, rep) |>
  filter(t == max(t)) |> 
  group_by(escapement) |>
  summarise(mean_reward = mean(reward), sd = sd(reward)) |> 
  filter(mean_reward == max(mean_reward))
}

# Collect data (outsource to separate R script later TBD)
msy_df <- read_csv("../data/msy.csv.xz")
best_action <- msy_optimizer(msy_df)$action
escapement_df <- read_csv("../data/escapement.csv.xz", show_col_types = FALSE)
best_e <- escapement_optimizer(escapement_df)$escapement
ppo_df <- read_csv("../data/PPO200.csv.xz")

opt_escapement <- escapement_df |> filter(escapement == best_e)

# set up for plots:

msy_sim <- msy_df |>
    filter(action == best_action) |> 
    group_by(rep) |>
    pivot_longer(c("X", "Y", "Z"),
                names_to = "species", values_to = "abundance") |>
    mutate(abundance = abundance + 1, policy = "msy") # natural units, policy name

actions <- unique(msy_df$action)
i <- which.min(abs(actions - best_action* 0.8))
tac_sim <- msy_df |> 
    filter(action == actions[[i]]) |>
    pivot_longer(c("X", "Y", "Z"),
        names_to = "species", values_to = "abundance") |>
    mutate(abundance = abundance + 1, policy = "tac")

ppo_sim <- ppo_df |> 
    pivot_longer(c("X", "Y", "Z"),
        names_to = "species", values_to = "abundance") |>
    mutate(abundance = abundance + 1, policy = "ppo")

best_e <- 0.61 # optimal constant escapement, computed from ../data/escapement.csv.gz
esc_sim_df <- escapement_df |> 
    pivot_longer(c("X", "Y", "Z"),
        names_to = "species", values_to = "abundance") |>
    mutate(abundance = abundance + 1)

msy_tac_ppo <- bind_rows(msy_sim, tac_sim, ppo_sim)
```

<!--
 Special Issue Description:

This special issue highlights the development of novel data-driven methods, including statistics, machine learning, parameter estimation, and uncertainty quantification, and combinations thereof, towards modeling biological systems. These newly developed methods will tackle challenges that are commonly encountered when modeling real-world experimental, field, pre-clinical, or clinical data. Examples of such challenges include high dimensionality, computational complexity, observation or process error, model bias, and intra- or inter-individual heterogeneity. Contributions to this special issue require validation of new methods with real-world data or simulated data sets that contain features of real-world data that exemplify an outlined modeling challenge. Papers should include a discussion justifying why the developed method is novel and not an application of previously developed methods, as well as how the method may be broadly applicable across different areas of biology, including medical, ecological, genetics, and epidemiological applications.
-->

# Introduction {#intro}

<!--
Existing methods for optimal control methods struggle to deal with the complexity commonly encountered in real-world systems, including dimensionality, process error, model bias and heterogeneity. 
Instead of tackling these complexities directly, researchers have typically sought to find exact optimal solutions to simplified models of the processes in question. 
When is the optimal solution to a very approximate, stylized model better than an approximate solution to a more accurate model? 
This question has largely gone unanswered owing to the difficulty of finding even approximate solutions in the case of complex models.  
Our approach draws on recent algorithmic and computational advances in deep reinforcement learning. 
These methods have hitherto focused on problems in games or robotic mechanics, which operate under precisely known rules. 
We demonstrate the ability for novel algorithms using deep neural networks to successfully approximate such solutions (the "policy function" or control rule) without knowing or ever attempting to infer a model for the process itself. 
This powerful new technique lets us finally begin to answer the question.
We show that in many but not all cases, the optimal policy for a carefully chosen over-simplified model can still out-perform these novel algorithms trained to find approximate solutions to simulations of a realistically complex system. 
This illustrates the promise of combining complex, realistic simulations with emerging machine-learning techniques to improve our understanding and management of ecological systems.
-->

Ecological management may be understood as an optimal control problem - a manager must determine a sequence of actions (a "policy") to optimize some desired objective(s) and constraints (e.g. conservation goals, natural resource extraction, economic and social constraints.)
Despite the complexity inherent in natural ecosystems, analytically tractable models have long played an essential role in their understanding and management.
...


<!-- Why fisheries-focused examples -->
Global fisheries management has long been a crucible for both the development of theory and best practices in actual management for many reasons. Fisheries are an essential natural resource that provide the primary source of protein for one in every four humans, and have faced widely documented declines due to overfishing [@Worm2006, @Costello2016]. 
Because of this importance, fisheries also represent some of the longest and best studied timeseries of population dynamics available to ecologists, as well as a correspondingly rich area for the development of theory and practice.
Methods developed in the study and management of fisheries are frequently applied to other areas of ecological management, from concepts such as maximum sustainable yield [], optimal control under observation uncertainty [@Memarzadeh2019], to adaptive management [@Walters], management strategy evaluation [@Punt], to the studies of conservation policy and international negotiations [@].

<!-- A sweeping summary of fisheries decision theory, don't bite me-->
To understand the importance of a potential application of novel data-driven methods to the real world, it is most instructive to have a clearer picture of how models are currently used to inform ecological management in fisheries.
In any issue as large and contentious as fisheries management, it may be far more difficult to provide a generalization  about how things actually work than to make sweeping recommendations about how it ought to.
All the same, it is our firm belief that any researcher seeking to do the latter should first attempt to understand the former, so we beg the forbearance of any specialists in the practice as we seek to provide a concise overview of fisheries management today.
Here, we focus specifically on practices in data rich, 'well-managed' fisheries, which, despite some issues, may be among the best examples of ecological management at continental scale that is largely driven by scientific processes, models and data. 

<!-- Fisheries population models are rich but decisions aren't, mostly constant mortality -->
While fisheries stock assessment models are quite rich, the choice of possible strategies remains motivated by intution from much simpler, analytically tractable one-dimensional models.
This fact is not always obvious at cursory inspection, and can be met with cynicism even among professional fisheries managers -- after all, typical stock assessment models have over 100 parameters (e.g. see the stock assessments referenced from in @RAMLegacy) and best practices such as management strategy evaluation (MSE) evaluate and select potential policies by comparing across large and diverse simulations [e.g. @Punt2016].
However, the space of policies considered under such approaches is necessarily far smaller than the set of all possible action sequences.
Instead, fisheries management approaches such as MSE rely on theory based on more tractable models to restrict the search space of possible policies.  
Specifically, most fisheries policies are framed in terms of a *constant mortality* policy, (sometimes a simple piece-wise linear function, i.e. a single linear segment, is introduced as a precaution a very low stock sizes), where fishing 'mortality' is defined as the ratio of harvest (catch) to stock size, $F = H_t/x_t$.

The intuition for a constant mortality strategy is simple and well established 

Constant mortality policies have a strong basis in theory as well as practical justifications.


<!-- In contrast to constant mortality, the possible decision space is huge-->
The management of any ecological system can be framed in terms of a Sequential Decision Process - 
the manager observes the ecosystem state ($x_t$) and decides on action ($a_t$) to take (including potentially no action), potentially receiving any costs or benefits associated with that action and and state (reward $r_t$  $r_t = U(x_t,a_t)$),
while the system evolves into a new state in response $x_{t+1} = f(x_t, a_t)$. When the probability of the future state can be determined by the current state and action, we say the system is Markovian and the process a Markov Decision Process (MDP).
As in a game of chess, the space of possible sequences of actions a manager might choose over any realization of the Markov process is enormous.
When the action and state space are discrete and sufficiently small, computational approaches such as dynamic programming may be used to guarantee an exhaustive search for a single optimal policy.
Dynamic programming scales very poorly to larger state and action spaces.

Deep reinforcement learning has opened up increasingly compelling alternative 

Effort $Q = E X_t$  

Fisheries policies are typically implemented in terms of *catch quotas*, i.e. a limit to the number of metric tonnes of fish that can be caught during a given season. 


and/or *effort controls*, restrictions on the fishing effort -- in numbers of boats, time of year, or days at sea




# Figures brainstorm

Figure 0: contrast 1D vs 3D strategies (see commented out diagram).

```{r}

#| label: conceptual
#| echo: FALSE
#| message" FALSE
#| fig.align: 'center'
#| fig.cap: '(A) Classical management strategies often have the common aspect of reducing the complex dynamics of the fishery ecosystem to a single equation governing the harvested population (say, $V1$). where the interaction between $V1$ and its environment is summarized to a set of parameters, such as the growth rate $r$, and the carrying capacity $K$. An advantage of one-dimensional approaches is that the optimal policy is often known exactly, and, moreover, is intuitive. For example, in the logistic equation shown, the maximal sustainable yield of the system is attained at K/2. 
#| (B) RL allows us to solve for quantitative decision strategies considering more complex ecosystem dynamics. For example, in this paper, we find harvest strategies for a three-species model using a model-free RL algorithm, PPO. 
#| (C) We show how the RL-based solution that considers all three species finds harvest policies with higher average reward and with (D) lower probability of harvest species collapse than the classical approaches, only considering the dynamics of V1.', 
#| out.width: '1\\linewidth', 
#| fig.pos: 'H'
 
#knitr::include_graphics("Figures/conceptual-figure.png")
```


<!-- Reality __
  |        \
  v         \
1-D Model   More expressive model
  |                  |
  v                  v
Optimal        Suboptimal, but
solution   well-performing solution -->

Figure 1: 1-D and 3-D model conceptual figure. something about the objective / decision
Figure 2: Stability / multistability in the 1D and 3D models.  state space + time views
Figure 3: The 1-D optimal management solution.  'constant escapement' intuition etc

RL figures:
- schematic of RL (a-la previous marcus paper Fig 1)
- Neural network optimization figure

Results figures:
- timeseries example of management under the RL policy.  probably compare to managing under 1-D solution / rule-of-thumb methods
- state-space view of management doughnut 
- visualization / encapsulation of the policy heatmaps, slices, policy vs position along ellipse 
- reward plot over time (comparing methods)

# Mathematical models of fisheries

In this section we introduce some models describing the population dynamics of fisheries.
In general, the class of models that appear in this context are *first order finite difference equations* (potentially stochastic difference equations).
For $n$ species, these models have the general form

\begin{align}
  \label{eq:general model}
  \Delta N_t := N_{t+1} - N_t = f(N_t) - F^T N_t,
\end{align}
where $N_t = (X_t,\ Y_t,\ \dots) \in \mathbb{R}^{n}_+$ is a vector of populations, $f:\mathbb{R}^{n}\to\mathbb{R}^{n}$ is an arbitrary function, where the product of the mortality and the stock size, $F^T N_t$ is the harvest size at timestep $t$. 
Here, $F\in\mathbb{R}^n$ is a vector of fishing intensities, where $F_i$ is the intensity with which the $i$-th species is fished.
We will focus on the case where only the first species is fished, $F= (F_1,\ 0,\ \dots,\ 0)$.

## A single species model

Optimal control policies for fisheries are frequently based on 1-dimensional models, $n=1$.
The most familiar model of $f(X)$ is that of *logistic growth*, for which

\begin{align}
  \label{eq:logistic}
  f(X_t) = r X_t\big(1 - X_t / K \big) =: L(X_t;\ r, K).
\end{align}

The model this $f$ gives rise to is known as the Gordon-Schaefer model, after foundational work which first developed the concept of maximum sustainable yield, MSY, [@Gordon1954; @Schaefer1954], which remains a central concept in fisheries management.
In this model, the maximum growth rate occurs at population size of $N = K/2$.
This value is approached asymptotically, from any positive initial state $N_0 \in (0,K)$, under a constant mortality $F = r/2$ policy.
At equilibrium this produces an asymptotic harvest of $rK/4$ per time step---the maximum yield sustainable indefinitely.




Real world ecological systems are obviously far more complicated than this simple model suggests.
One particularly important aspect that has garnered much attention is the potential for the kind of highly non-linear functions that can support dynamics such as alternative stable states and hysteresis.
A seminal example of such dynamics was introduced in \cite{may77}, using a one-dimensional model of a prey (resource) species under the pressure of a (fixed) predator:

\begin{align}
  \label{eq:may}
  f_{\text{May}}(X_t)
  := L(X_t;\ r, K) - F(X_t, H; \beta,c),
\end{align}
where,
\begin{align*}
  F(X_t,H;\ \beta,c) := \frac{\beta H X_t^2}{c^2 + X_t^2}.
\end{align*}

The model has five parameters: the growth rate $r$ and carrying capacity $K$ for $X$, a constant population $H$ of a species which preys on $X$, the maximal rate of predation $\beta$, and the predation half-maximum biomass $c$.

Eq. \eqref{eq:may} is an interesting study case of a *tipping point* (saddle-node bifurcation) (see Fig. \ref{fig:may stable}).
Holding the value of $\beta$ fixed, for intermediate values of $H$ there exist two stable fixed points for the state $\hat X_t$ of the system, these two attractors separated by an unstable fixed point.
At a certain threshold value of $H$, however, the top stable fixed point collides with the unstable fixed point and both are annihilated.
For this value of $H$, and for higher values, only the lower fixed point remains.
This also creates the phenomenon of *hysteresis*, where returning $H$ to its original value is not sufficient to restore $\hat X_t$ to the original stable state. 

\begin{figure}
  \centering
  \includegraphics[width = 0.8\linewidth]{figures/MayStablePoints.pdf}
  \caption{
    \label{fig:may stable}
    The fixed point diagram for the model \cite{may77} associated to $f_{\text{May}}$ as a function of varying the parameter $\beta H$.
    That is, these are the zeroes of the function $f_{\text{May}}$. 
    Stable fixed points (also known as *attractors*) are plotted using a solid line, while the unstable fixed point is shown as a dotted line.
  }
\end{figure}

This structure implies two things.
First, that a drift in $H$ could lead to catastrophic consequences, with the population $X_t$ plummeting to the lower fixed stable point.
Second, that if the evolution of $X_t$ is *stochastic*, then, even at values of $H$ below the threshold point, the system runs a sizeable danger of tipping over towards the lower stable point.


<!-- Discuss control solutions of this model(?) -->
<!-- Briefly motivate reason to consider 3d model, adding competition and predation processes -->


## A three species model

Consider the following three species *generalization* of eq. \eqref{eq:may}:

\begin{align}
\label{eq:3d model}
\begin{split}
  \Delta X_t &= L(X_t;\ r_X, K_X) - F(X_t, Z_t;\ \beta, c) - c_{XY}X_tY_t - h_t,\\
  \Delta Y_t &= L(Y_t;\ r_Y, K_Y) - D F(Y_t, Z_t;\ \beta, c) - c_{XY}X_tY_t,\\
  \Delta Z_t &= \left(f(X_t + DY_t\right) - d_Z)Z_t.
\end{split}
\end{align}
The model is conceptualized in Fig. \ref{fig:conceptual}, panel B.
It contains three populations, the state at time $t$ is $(X_t, Y_t, Z_t)$. 
Species $Z$ preys on both $X$ and $Y$, while the latter two compete for resources.
There are ten parameters in this model: 
The growth rate and carrying capacity, $r_X$, $K_X$, $r_Y$ and $K_Y$, of $X$ and $Y$.
A parameter $c_{XY}$ mediating a Lotka-Volterra competition between $X$ and $Y$.
A maximum hunting intensity $\beta$ and half-maximum $c$ specifying how $Z$ forages on $X$ and $Y$.
A parameter $D$ regulating a relative preference of $Z$ to prey on $Y$.
Finally, a death rate $d_Z$ and a parameter $f$ related to the birth rate of $Z$.

This model generalizes eq. \eqref{eq:may} in the following sense:
If $f=d_Z=c_{XY}=0$, and we let $Z_{t=0}=H$, then eq. \eqref{eq:3d model} becomes
\begin{align*}
  \Delta X_t &= L(X_t;\ r_X, K_X) - F(X_t, H;\ \beta, c) - h_t,\\
  \Delta Y_t &= L(Y_t;\ r_Y, K_Y) - D F(Y_t, H;\ \beta, c),\\
  Z_t &= H.
\end{align*}
The equations for $X_t$ and $Y_t$ thus become decoupled, with $X_t$ evolving according to \eqref{eq:may}.

As can be expected, the dynamics of eq. \eqref{eq:3d model} are significantly more complex than those of eq. \eqref{eq:may}.
There are a variety of fixed points for the equations---around 7 or 8 for wide ranges of parameter values.
Fig. \ref{fig:tbd} provides a rough idea of the fixed point structure by plotting the bifurcation diagram obtained from varying $D$ and fixing the other parameters.

Throughout most of this paper we will fix the parameters of the model according to Table \ref{tab:tbd}.
At these values, the fixed points of the model are:
\begin{align*}
  TBD
\end{align*}
The classification of fixed points for multi-dimensional systems is generally more complicated than in one dimension.
A fixed point might be stable against perturbations in one direction, but unstable against perturbations along another.
While a detailed classification of these fixed points is beyond the scope of this work, we point out that the point $(TBD)$ is stable against perturbations in all directions.
We will thus use it as a starting point for our simulations.

## Stochastic dynamics

We have, up to now, presented eqs. \eqref{eq:may} and \eqref{eq:3d model} as *deterministic* dynamical models for the sake of clarity.
In practice, we will consider a stochastic contribution to these dynamics.
This way, instead of eq. \eqref{eq:may}, we consider the following stochastic single-species system
\begin{align}
  \label{eq:may stoch}
  \Delta X_t
  = L(X_t;\ r, K) - F(X_t, H; \beta,c) - h_t + \eta_t,
\end{align}
where $\{\eta_t\}_t$ are independent Gaussian variables with zero mean.
The variance $\sigma^2$ of $\eta_t$ is an additional parameter of the model.

Similarly, we use the following dynamical model in place of eq. \eqref{eq:3d model}
\begin{align}
  \label{eq:3d model stoch}
  \begin{split}
    \Delta X_t &= L(X_t;\ r_X, K_X) - F(X_t, Z_t;\ \beta, c) - c_{XY}X_tY_t - h_t + \eta_{X,t},\\
    \Delta Y_t &= L(Y_t;\ r_Y, K_Y) - D F(Y_t, Z_t;\ \beta, c) - c_{XY}X_tY_t + \eta_{Y,t},\\
    \Delta Z_t &= \left(f(X_t + DY_t\right) - d_Z)Z_t + \eta_{Z,t}.
\end{split}
\end{align}
Here, again, $\{\eta_{i,t}\}$ is a set of independent Gaussian variables centered at zero with variances, respectively, $\sigma_X^2$, $\sigma_Y^2$ and $\sigma_Z^2$.


```{r}
msy_rewards <- msy_df |>
  group_by(action, rep) |>
  filter(t == max(t)) |> 
  group_by(action) |>
  summarise(mean_reward = mean(reward),
            sd = sd(reward)) 

esc_rewards <- escapement_df |>
  group_by(escapement, rep) |>
  filter(t == max(t)) |> 
  group_by(escapement) |>
  summarise(mean_reward = mean(reward),
            sd = sd(reward)) 

rmsy <- msy_rewards |>
  ggplot(aes(action, mean_reward, 
             ymin = mean_reward - 2*sd,
             ymax=mean_reward+2*sd)) +
  geom_point() + geom_ribbon(alpha=0.4) +
  labs(y = "episode reward", x = "constant mortality") +
  theme(aspect.ratio = 1/1) + 
  geom_vline(aes(xintercept=0.0360), color="red") + 
  geom_vline(aes(xintercept=0.0288), color="green") 

resc <- esc_rewards |>
  ggplot(aes(esc_level, mean_reward, 
             ymin = mean_reward - 2*sd,
             ymax=mean_reward+2*sd)) +
  geom_point() + geom_ribbon(alpha=0.4) +
  labs(y = "episode reward", x = "constant escapement") +
  theme(aspect.ratio = 1/1)

# (rmsy | resc)
```

# Fishery management approaches

<!-- Introduce the 1D and 3D models, and have Figure 1 already here. -->

Here we review classical strategies for sustainable fishery management and we provide a birds-eye view of the alternative approach we propose.
A summary of the contrast between the two strategies is given in Fig. \ref{fig:decision approaches}.

## Classical approaches to sustainable fisheries {#sec:classical}

There are several strategies that have been used to manage fisheries: *escapement*, *maximum sustainable yield (MSY)*, *total allowable catch (TAC)*, among others.
We collectively refere to these as *classical*, and will compare their performance to RL-based management strategies.
Classical strategies derive an optimal harvest policy using a simple model for the system dynamics.
While often complex models of the ecosystem are used to estimate the fish population of interest (i.e. $X_t$), the quota is set only taking into account the estimate $X_t$.
The function determining this quota is derived from a simple dynamical model for the population (see text box 1).

\textbf{Text box 1 (TBD formatting)}
Let us consider a ubiquitous simple example: an MSY policy based on a logistic growth model for $X_t$ (see eqs. \eqref{eq:general model} and \eqref{eq:logistic}).


Classical strategies have the common aspect of reducing the complex dynamics of the fishery ecosystem to a single equation governing the harvested population (say, $F$). 
A common example is using a logistic growth equation,
$$
  F_{t+1} - F_t = r F_t( 1 - F_t / K) - h_t =: L(F_t) - h_t,
$$
where the interaction between $F$ and its environment is summarized to two parameters, the growth rate $r$, and the carrying capacity $K$.
In the equation above, $h_t$ is the *harvest* at timestep $t$.
The goal is to choose the harvest policy $h:F_t\mapsto h_t$ such that long-term profits are maximized.

An advantage of one dimensional approaches is that the optimal policy is often known exactly, and, moreover, is intuitive.
For example, in the logistic equation pointed out above, the maximal sustainable yield of the system is attained at $F = F_{MSY} := K/2$.
The optimizer is an *escapement* policy:
$$
  h_t = \begin{cases}
    F_t - K/2, \qquad &\text{if, } F_t > K/2\\
    0, \qquad &\text{else.}
  \end{cases}
$$
This corresponds to keeping the system at its optimal growth rate as much as possible.

Escapement policies, or more generally *bang bang* policies, tend to be the optimal solution for these types of control problems.
A drawback of these solutions, in the fishery context, is the presence of several timesteps with zero harvest which can arise.
To mend this, certain suboptimal solutions have been constructed for fishery management.

One ubiquitous solution is simply called maximum sustainable yield (MSY).
It consists on letting $h(F) = rF/2$, so that
$$
  h(F_{MSY}) = L(F_{MSY}) = r K/4.
$$
That is, at the MSY biomass, the logistic growth of $F$ is cancelled exactly by the harvest.

The MSY rule fixes the drawback in the escapement policy by having $h(F)>0$ for all $F>0$.
It, however, has its own drawbacks.
It is particularly sensitive to misestimates of the parameter $r$, as we will discuss in Sec. \ref{sec:tbd}.
Due to this, similar but more conservative policies have been used.

*Total allowable catch (TAC)* is one such policy.
It consists on reducing the inclination of the line defined by $h(F)$ using a prefactor $\alpha$ in $h(F) = \alpha\times rF/2$. 
Plausible examples are $\alpha = 0.8$ or $0.9$.
Another common alternative is to have a prefactor $\alpha = \alpha(F)$ which decreases from one to zero as $F$ decreases below a threshold:
$$
  h(F) = \alpha(F)\times rF/2,
$$
where,
$$
  \alpha(F) = \begin{cases}
  1, \qquad &\text{if } F > F_{\text{thresh.}},\\
  F / F_{\text{thresh.}}, \qquad & \text{else}.
  \end{cases}
$$
We call this policy *variable total allowable catch (VTAC)*.

## Dangers: Noisy parameters, noisy observations and model inaccuracy


# Reinforcement learning {#sec:RL}

Reinforcement learning (RL) is, in a nutshell, a way of approaching *control problems* through machine learning.
An RL algorithm can be conceptually separated into two parts: an *agent*, and an *environment* which the agent can interact with.
That is, the agent may act on the environment and thus change its state, while the environment gives a *reward* to the agent in return (see Fig. \ref{tbd}).
The rewards encode the agent's goal.
The main part of an RL algorithm is then to progressively improve the agent's *policy*, in order to maximize the cumulative reward received. 
This is done by aggregating experience and learning from it.

<!-- The *environment* is commonly a computer simulation, although it sometimes can be a real world system.
The *agent*, on the other hand, is a computer program which interacts with the environment. -->

For our use case, the environment will be a computational model of the population dynamics of a fishery, with the environment state being a vector of all the fish populations, $S=(V_1, V_2, H)$.
At each time step, the agent harvests one of the populations, $V_1$. 
This changes the state as
\begin{align*}
  (V_1,\ V_2,\ H) \mapsto ((1-q)V_1,\ V_2,\ H),
\end{align*}
where $q$ is a fishing *quota* set by the agent. 
This secures a reward of $qV_1$.
Afterwards, the environment undergoes a timestep under its natural dynamics given by \eqref{eq:3d model}.

## Mathematical framework for RL

Mathematically, RL may be formulated using a discrete time *partially observable Markov decision process (POMDP)*.
This formalization is rather flexible and allows one, e.g., to account for situations where the agent may not fully observe the environment state, or where the only observations available to the agent are certain functions of the underlying state.
For the sake of clarity, we will present here only the class of POMDPs which are relevant to our work: *fully observable MDPs with a trivial emission function* (FMDPs for short).
An FMDP may be defined by the following data:

- $\mathcal{S}$: *state space*, the set of states of the environment,

- $\mathcal{A}$: *action space*, the set of actions which the agent may choose from,

- $T(s_{t+1}|s_t, a_t)$: *transition operator*, a conditional distribution which describes the dynamics of the system,

- $r(s_t, a_t)$: *reward function*, the reward obtained after performing action $a_t\in\mathcal{A}$ in state $s_t$,

- $d(s_0)$: *initial state distribution*, the initial state of the environment is sampled from this distribution,

- $\gamma\in[0,1]$: *discount factor*.

At a time $t$, the FMDP agent observes the full state $s_t$ of the environment and chooses an action based on this observation according to a *policy function* $\pi(a_t | s_t)$.
In return, it receives a discounted reward $\gamma^t r(a_t, s_t)$.
The discount factor helps regularize the agent, helping the optimization algorithm find solutions which pay off within a timescale of $t \sim \log(\gamma^{-1})^{-1}$.

With any fixed policy function, the agent will traverse a path 
$\tau=(s_0,\ a_0,\ s_1,\ a_1\ \dots,\ s_{t_{\text{fin.}}})$ 
sampled randomly from the distribution
\begin{align*}
  p_\pi(\tau) = 
  d(s_0) \prod_{t=0}^{ t_{\text{fin.}}-1 }
  \pi( a_t | s_t ) T( s_{t+1} | s_t, a_t ).
\end{align*}
Reinforcement learning seeks to optimize $\pi$ such that the expected rewards are maximal,
$$  
  \pi^* = \mathrm{argmax}\ \mathbb{E}_{\tau\sim p_\pi}[R(\tau)],
$$
where,
$$
  R(\tau) = \sum_{t=0}^{ t_{\text{fin.}}-1 } \gamma^t r(a_t, s_t),
$$
is the cumulative reward of path $\tau$.
The function $J(\pi):=\mathbb{E}_{\tau\sim p_\pi}[R(\tau)]$ is called the *value function*.

## Deep Reinforcement Learning

The policy function $\pi$ often lives in a high- or even infinite-dimensional space.
This makes it unfeasible to directly optimize $\pi$.
In practice, an alternative approach is used: $\pi$ is optimized over a much lower-dimensional parametrized family of functions.
Deep reinforcement learning uses this strategy, focusing on function families parametrized by neural networks. (See Fig. \ref{tbd}.)
<!-- The figure should kinda explain how the parametrization arises, and how gradient ascent is used. Maybe namedrop backpropagataion for the sake of usefulness to the reader? -->

Since our state and observation spaces are continuous, we will focus on deep reinforcement learning throughout this paper.
Specifically, we parametrize $\pi$ using a neural network with two hidden layers of 64 neurons.

We use the *proximal policy optimization (PPO)* algorithm to optimize $\pi$.
Within the RL literature there is a wealth of algorithms from which to choose from, each with its pros and cons.
Here we have focused on a single one of these for the sake of particularity---to draw a clear comparison between the RL-based and the classical fishery management approaches.
In practice, further improvements can be expected by a careful selection of the optimization algorithm.

## Model-free reinforcement learning

Within control theory, the usual setup is one where we use as much information from the model as possible in order to derive an optimal solution.
**(Wax poetic a bit about Bellmann eq. approaches here?)**

The classical sustainable fishery management approaches summarized in Sec. \ref{sec:classical} are model-based controls.
As we saw in that section, these controls may run into trouble in the case where there are inaccuracies in the model parameter estimates.

More generally, there are many situations in which the exact model of the system is not known or not tractable.
This is a standard situation in ecology: mathematical models capture the most prominent aspects of the ecosystem's dynamics, while ignoring or summarizing most of its complexity.
In this case, it is clear, model-based controls run a grave danger of mismanaging the system.

Reinforcement learning is, on the other hand, typically a model-free approach to control theory.\footnote{There are, however, approaches to perform model-based reinforcement learning. While we will not focus on these in this paper they are discussed in \cite{tbd1, tbd2}.}
While the model is certainly used to generate training data, it is not directly used by the optimization algorithm.
This provides more flexibility to use model-free RL in instances where the model of the system is not accurately known.
In fact, it has been shown to generally be preferable to use model-free RL in such instances.

**Mention CL here? Where solutions to other related problems in the curriculum are used to build solutions to the target problem.**

This context provides a motivation for this paper. 
Indeed, models for ecosystem dynamics are only ever approximate and incomplete descriptions of reality. 
This way, it is plausible that model-free RL controls outperform currently used model-based controls in ecological management problems.


# Methods

\textbf{Training a DRL agent.}
We trained a DRL agent parametrized by a neural network $\theta$ with two hidden, 64-neuron, layers.
The state space used was a cube $[0,2]^{\times 3}$, while the policies explored mapped states $N_t$ to a single number, $h_t = \pi_\theta(N_t)$, the harvest quota on species $X$ at that timestep.
Each episode in the training and in the evaluation stages was at most 200 time steps long.
Episodes were ended early in case of a ‘‘near-extinction event,’’ a moment in which any of the populations dipped below a threshold value, i.e.,
\begin{align}
  \label{eq:thresh}
  X_t \leq X_{\text{thresh.}},\quad
  Y_t \leq Y_{\text{thresh.}},\quad
  \text{or,}\quad
  Z \leq Z_{\text{thresh.}}.
\end{align}
The reward function had two componenets:
First, at each time step the harvest $h_t$ was received as a reward.
Second, if the episode ended early (say, at time-step $t$) because of a near-extinction event, a negative reward of $-50/t$ was awarded.
The constant 50 was chosen so as to select against highly extractive policies in the early stages of training, and thus guide the agent towards sustainable policies.
This technique, falling under the ‘‘umbrella’’ category of *reward shaping*, is commonly used -- alongside hyper-parameter tuning and state space normalization -- to improve the speed of training by avoiding badly-performing local optima in the gradient ascent algorithm.

Thus, we normalized state space (to be contained in a cube of length 2) and engaged in reward shaping in order to optimize our agent's learning.
We did not need to tune hyper-parameters for this goal, however.

We used the Ray framework [@ray_package] for training, in particular we used a Ray PPOConfig object to build the algorithm using the default values for all hyper-parameters (cf. [@repo] for the code used).
The agent was trained for 250 iterations of the PPO optimization step.

\textbf{Tuning the constant mortality strategy.}
A grid of 200 mortality rates was laid in the interval $[0, 0.1]$.
For each one of these rates, say $F$, we simulated 50 episodes based on \eqref{eq:3d model stoch}:
At each time step the state $(X_t, Y_t, Z_t)$ was observed, a harvest of $FX_t$ was collected and then the system evolved according to its natural dynamics \eqref{eq:3d model stoch}.
The optimal mortality rate was the value $F^*$ for which the mean episode reward was maximal.
We additionally evaluated a more conservative strategy where a mortality of $0.8 F^*$ instead is used.

The result of the tuning procedure is shown in Fig. \ref{fig:linear_tuning}. 
There, we see that the optimal value $F^*$ lies in an area with high episode reward variance.
We will see in Fig. \ref{fig:rew_len_fig} that a likely candidate for this variance is the presence of episodes with early endings.
Thus, the value $0.8 F^*$ supports a smaller variance, and correspondingly -- as Fig. \ref{fig:rew_len_fig} shows -- this policy leads to a much smaller probability of a near-extinction event.

```{r}
#| label: linear_tuning, 
#| fig.cap : "Result of the tuning procedure for the constant mortality strategy. 
#| (A) Mean episode reward (averaged over 50 episodes) shown as scatter data, and shaded in gray are two standard deviations around each scatter point. Vertical lines correspond to the optimal constant mortality rate $F^* = $ TBD (red) and $0.8 F^*=$ TBD (green). 
#| (B) Several constant mortality policies shown for illustrative purposes. Slopes $0.02$, $0.03$, $0.055$ and $0.065$ shown in black; the optimal linear policy $F^*X$ shown in red, while $0.8F^*X$ shown in green." 
#| out.width : "6.5in"
msy_rewards <- msy_df |>
  group_by(action, rep) |>
  filter(t == max(t)) |> 
  group_by(action) |>
  summarise(mean_reward = mean(reward),
            sd = sd(reward)) 

rmsy <- msy_rewards |>
  ggplot(aes(action, mean_reward, 
             ymin = mean_reward - 2*sd,
             ymax=mean_reward+2*sd)) +
  geom_point() + geom_ribbon(alpha=0.4) +
  labs(y = "episode reward", x = "constant mortality") +
  theme(aspect.ratio = 1/1) + 
  geom_vline(aes(xintercept=best_action), color="red") + 
  geom_vline(aes(xintercept=0.8*best_action), color="green") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

# linear function blueprint
linear_f <- function(slope, x) slope*x
# generates linear function with given slope
linear_f_gen <- function(slope) (function(x) slope*x)

# dummy data set to keep ggplot happy
p <- ggplot(data = data.frame(x = 0), mapping = aes(x = x))

# p + stat_function(fun = linear_f_gen(0.5)) + xlim(0,2)
lines <- p + stat_function(fun = linear_f_gen(0.01)) + 
  stat_function(fun = linear_f_gen(0.02)) +
  stat_function(fun = linear_f_gen(0.03)) +
  stat_function(fun = linear_f_gen(0.8*best_action), col = "green") +
  stat_function(fun = linear_f_gen(best_action), col = "red") +
  stat_function(fun = linear_f_gen(0.055)) + 
  stat_function(fun = linear_f_gen(0.065)) +
  xlim(0,2) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())


(rmsy | lines) + plot_annotation(tag_levels = c('A'))
```

\textbf{Parameter values used.}
The model's dynamic parameters were chosen as follows
\begin{align*}
  r_X = K_X = r_Y = K_Y = 1, \quad \beta = 0.3, \quad c = 0.1\\
  c_{XY} = 0.5,\quad f = 0.15,\quad D = 1.1, \quad d_Z = 0.135.
\end{align*}
Moreover, the variances for the stochastic terms were chosen as
$$
  \sigma^2(\eta_{X,t}) = \sigma^2(\eta_{Y,t}) = \sigma^2(\eta_{Z,t}) = 0.05.
$$
The threshold values used to determine an early episode end were chosen as
$$
  X_{\text{thresh.}} = 0.08,\quad Y_{\text{thresh.}} = Z_{\text{thresh.}} = 10^{-4},
$$
where the higher value of $X_{\text{thresh.}}$ helped select for more sustainable fishing strategies (for both the DRL and constant mortality approaches).

# Results

We evaluated each of the three control approaches (MSY constant mortality, 80% MSY constant mortality and DRL) by running 50 episodes using the respective control rule.
In Fig. \ref{fig:timeseries} we show the time series of one such repetition.
The left panel uses an MSY constant mortality control $F^*X$, the middle panel uses $0.8 F^* X$, while the right panel is controlled by the trained DRL agent.
We see that that optimal constant mortality control leads to a near-extinction event at around $t=150$.
The constant mortality policy at $0.8 F^*$ is already conservative enough to support a sustainable path.
There, the population levels are maintained roughly in a stationary state.
Finally, the DRL time series shows an oscillatory behavior, with a slight dephase between the $Y$ and $Z$ curves, while the $X$ oscillations are roughly anticorrelated with $Z$.

```{r} 
#| label: timeseries 
#| fig.cap: "A sample time series for the three fish populations, with the system being controlled, respectively, by the MSY optimal mortality, by $80$ percent of the MSY optimal mortality, and by DRL." 
#| out.width: "6.5in"
policy_labels <- list("msy" = "MSY Const. Mortality", "tac"= "80% MSY Const. Mortality", "ppo"= "DRL")
policy_labeller <- function(variable,value){
  return(policy_labels[value])
}
msy_tac_ppo |> 
    group_by(rep, policy) |>
    filter(rep == 6) |>
    ggplot(aes(t, abundance, col = species)) + 
        geom_line()  + 
        scale_color_manual(values=c('palegreen3', 'coral1', 'deepskyblue4')) +
        facet_wrap(~factor(policy, levels = c("msy", "tac", "ppo")), labeller = policy_labeller ) +
        labs(y = "abundance", x = "time") +
        theme(aspect.ratio = 2/3)
```

This time series is representative of the patterns observed in the data (see App. \ref{app:tbd}).
The episode reward and episode length statistics are shown in Fig. \ref{fig:rew_len_fig}.
There, we highlight two things:
First, that the tendency towards early episode ends is fixed by the conservative approach of fishing at 80% of the optimal mortality rate.
(In App. \ref{app:tbd} we discuss a warning about this approach, however: the arbitrarily chosen percentage itself lives on a ‘‘knife's edge.’’
More concretely, choosing a higher percentage, say 90%, can switch the behaviour of the controlled system and give rise to the proliferation of early episode ends.)
Second, that while the conservative 80% approach has a stable behavior, it consistently produces smaller harvests than a DRL generated policy.

```{r} 
#| label: rew_len_fig
#| fig.cap: "Bar charts for the episode lengths (top panel) and episode total harvests (bottom panel) for 50 episodes." 
#| out.width: "6.5in"
p1 <- msy_tac_ppo |> 
    group_by(rep, policy) |>
    filter(t==max(t), species == "X", rep < 50) |>
    ggplot(aes(rep, t)) + 
      geom_col() +
      facet_wrap(~factor(policy, levels = c("msy", "tac", "ppo")), labeller = policy_labeller ) +
      labs(y = "episode length", x = "repetition") +
      theme(axis.text.x = element_blank())

p2 <- msy_tac_ppo |> 
    group_by(rep, policy) |>
    filter(t==max(t), species == "X", rep < 50) |>
    ggplot(aes(rep, reward)) + 
      geom_col() +
      facet_wrap(~factor(policy, levels = c("msy", "tac", "ppo")), labeller = policy_labeller ) +
      labs(y = "episode harvest", x = "repetition") +
      theme(axis.text.x = element_blank())

(p1 / p2) + plot_annotation(tag_levels = 'A')
```

To understand the DRL-derived policy, Fig. \ref{fig:action-X} depicts the harvest as a function of $X$.
Specifically, for each point $(x,y,z)$ visited by one of the aforementioned 50 evaluation episodes, we evaluate the harvest collected by the DRL agent and we subsequently plot the scatter data as a function of the projection $x$ onto the $X$ axis.
In panel A we have used color to add the a $Y$-dependence, while in panel B we did the same with $Z$-dependence.
This shows that the $Z$ and $Y$ dependence of the policy is rather weak and in fact it has roughly the same shape as an escapement policy.

Fig. \ref{action-X} additionally shows (as a red line) the optimal escapement policy.
This policy was computed in a manner similar to how the MSY constant mortality was computed: for each escapement in a grid of 100 values between 0 and 1, 50 episodes were run using the corresponding escapement policy.
The value with the best mean episode reward was chosen as the optimal escapement value.


```{r}
#| label: action-X
#| fig.cap: "TBD"
#| out.width: "6.5in"
opt_escapement_harvest <- function(X) case_when(
  X > best_e ~ X - best_e,
  X <= best_e ~ 0
)

py <- ppo_df |>
  mutate(X = X+1, Y = Y+1, Z = Z+1, harvest = action*X) |>
  ggplot(aes(X, harvest, color = Y)) +
  geom_point() +
  theme(aspect.ratio = 1/1) +
  stat_function(fun = opt_escapement_harvest, col = 'red')

pz <- ppo_df |>
  mutate(X = X+1, Y = Y+1, Z = Z+1, harvest = action*X) |>
  ggplot(aes(X, harvest, color = Z)) +
  geom_point() +
  theme(aspect.ratio = 1/1) +
  stat_function(fun = opt_escapement_harvest, col = 'red')

(py | pz) + plot_annotation(tag_levels = c('A'))
```

In Fig. \ref{fig:state-space}, we perform a back to back visualization of all the policies evaluated up to now---MSY constant mortality, 80% MSY constant mortality, DRL and escapement.
This highlights the similarity between the policy found by the DRL agent and the optimal escapement policy.

# Old Methods and Results

## Methods

Model-based controls, such as MSY, are known to have serious deficiencies when the model used to find the optimal control does not match reality.
As mentioned, for the particular case of fisheries this can lead to depletion of resources and affect severely the surrounding ecosystems.
Because of this, we have opted to compare the performance of several data-based approaches to fishery control (see Fig. \ref{fig:decision approaches}).

As shown in Fig. \ref{fig:decision approaches}, these data-based strategies rely on choosing a family of *approximating functions* for the policy, $\mathcal{P}:=\{\pi_\theta\}_\theta$, parametrized by some vector $\theta$.
These functions are deterministic policies, i.e. they map states to actions, or, specifically,
$$
  \pi_\theta : \mathbb{R}^n \to \mathbb{R}_+,
$$
where, as before, $n$ is the number of species.
The aim of the optimization process is to find a parameter choice $\theta^*$ for which $\pi_{\theta^*}$ is optimal within $\mathcal{P}$.

Our focus here will be comparing two data-based approaches: *constant mortality* and *DRL*.
Constant mortality is, as the name suggests, based on MSY.

For the constant mortality approach, we choose a linear harvest $\pi_F(N_t)= F X_t$, where the parameter $F$ is scalar, and where $N_t = (X_t,\ \dots\ )\in \mathbb{R}^n$.
This first control is intended to delineate how one might apply MSY to a real-world fishery.
We will also compare a TAC-inspired rule of thumb which uses, instead, the parameter value $F_{\text{TAC}} = 0.8 F^*$.
In App. \ref{app:tbd}, we discuss several other MSY-based approaches which yield similar results to the strategies just discussed.

In the DRL approach, we instead parametrize $\mathcal{P}$ using a neural network with two hidden layers of 64 neurons.
Our hypothesis is that the additional flexibility yielded by neural networks will allow the agent solutions that are better than MSY based approaches.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/DecisionApproaches.pdf}
  \caption{
  \label{fig:decision approaches}
  TBD.
  }
\end{figure}

## Results

\textbf{Reword a bit.}
We trained a DRL agent to obtain a candidate policy
\begin{align*}
  \pi: (X_t,Y_t,Z_t) \mapsto h_t
\end{align*}
for eq. \eqref{eq:3d model}.
The agent could fully and precisely observe the state of the system at each time step and decide on its harvest based on this information. 
The agent was trained using the PPO algorithm, using a neural network with two hidden layers of 64 neurons to express $\pi$.
Details of the training process may be found in App. \ref{app:tbd}.
Our code is available at TBD.

Rewards were defined to be a compound of two contributions.
The first is economical: if at time $t$ a harvest of $h_t$ is secured, then the agents total reward is increased by $h_t$.
The second regards sustainability: if an extinction happens at time step $t$, then a penalty of $-50/t$ is incurred by the agent.
The choice of the constant $50$ in this rule was rather arbitrary: we set it to be a fourth of the maximal episode length ($200$ time steps), so as to encourage the agent to find policies that do not lead to extinctions below the time scale of around 100 time steps.
\textbf{Too long of a "sorry" message?}

We compare the performance of this agent to the performance of MSY and of an 80% TAC. 

Our results suggest that DRL can find solutions that are more sustainable *and* more profitable than any constant mortality strategy.
Indeed, DRL finds solutions in which, for certain periods of time, no harvest is performed on the system.
These periods effectively allow the system to, in a sense, "reset" itself.
They lead to a distinctive oscillating behavior of the system state over time, with periods of relative depletion followed by periods of growth. 
Figure TBD exemplifies this point by comparing one times series of the system state obtained using each of these policies.


```{r timeseries_old, fig.cap="TBD.", out.width = "6.5in"}
policy_labeller <- function(variable,value){
  return(policy_labels[value])
}
msy_tac_ppo |> 
    group_by(rep, policy) |>
    filter(rep == 6) |>
    ggplot(aes(t, abundance, col = species)) + 
        geom_line()  + 
        scale_color_manual(values=c('palegreen3', 'coral1', 'deepskyblue4')) +
        facet_wrap(~factor(policy, levels = c("msy", "tac", "ppo")), labeller = policy_labeller ) +
        labs(y = "abundance", x = "time") +
        theme(aspect.ratio = 2/3)
```


```{r rew_len_fig_old, fig.cap="TBD.", out.width = "6.5in"}
library(patchwork)

p1 <- msy_tac_ppo |> 
    group_by(rep, policy) |>
    filter(t==max(t), species == "X", rep < 50) |>
    ggplot(aes(rep, t)) + 
      geom_col() +
      facet_wrap(~factor(policy, levels = c("msy", "tac", "ppo")), labeller = policy_labeller ) +
      labs(y = "episode length", x = "repetition") +
      theme(axis.text.x = element_blank())

p2 <- msy_tac_ppo |> 
    group_by(rep, policy) |>
    filter(t==max(t), species == "X", rep < 50) |>
    ggplot(aes(rep, reward)) + 
      geom_col() +
      facet_wrap(~factor(policy, levels = c("msy", "tac", "ppo")), labeller = policy_labeller ) +
      labs(y = "episode harvest", x = "repetition") +
      theme(axis.text.x = element_blank())

(p1 / p2) + plot_annotation(tag_levels = 'A')
```

```{r state-space, fig.cap="TBD.", out.width = "6.5in"}
i_tac <- which.min(abs(actions - best_action * 0.8))

# color pallette limits
library(RColorBrewer)
myPalette <- colorRampPalette(rev(brewer.pal(11, "Spectral")))
sc <- scale_colour_gradientn(colours = myPalette(100), limits=c(0,0.25))

pmsy <- msy_df |>
  mutate(X = X+1, Y = Y+1, Z = Z+1, harvest = action * X) |>
  filter(action == best_action) |>
  ggplot(aes(X, Z, color = harvest)) +
  geom_point() + sc +
  ggtitle("Opt. Const. Mortality")

ptac <- msy_df |>
  mutate(X = X+1, Y = Y+1, Z = Z+1, harvest = action * X) |>
  filter(action == actions[[i_tac]]) |>
  ggplot(aes(X, Z, color = harvest)) +
  geom_point() + sc +
  ggtitle("80% Opt. Const. Mortality")

pppo <- ppo_df |>
  mutate(X = X+1, Y = Y+1, Z = Z+1, harvest = action * X) |>
  ggplot(aes(X, Z, color = harvest)) +
  geom_point() + sc +
  ggtitle("DRL")

pesc <- opt_escapement |>
  mutate(X = X+1, Y = Y+1, Z = Z+1, 
        harvest = case_when(
          X - escapement > 0 ~ X - escapement,
          X - escapement <= 0 ~  0)
         ) |> 
  ggplot(aes(X, Z, color = harvest)) +
  geom_point() + sc +
  ggtitle("Opt. Const. Escapement")

(
  (pmsy | ptac) / (pppo | pesc) +
  plot_layout(guides = "collect") & theme(legend.position = 'right')
) + 
  plot_annotation(tag_levels = c('A'))
```


```{r escapement_timeseries, fig.cap = "TBD", out.width = "6.5in"}

ppo_mod <- ppo_df |> 
  mutate(X = X+1, Y = Y+1, Z = Z+1, escapement = (1-action)*X, time = t)

avg_esc <- mean(ppo_mod$escapement)
# = 0.6201131

ppo_esc <- ppo_mod |>
  filter(rep == 6) |>
  group_by(rep) |>
  ggplot(aes(time, escapement), alpha = rep) +
  geom_line() +
  ylim(0,1) +
  geom_hline(aes(yintercept=avg_esc), color="red")+
  theme(aspect.ratio = 1/5)

ppo_esc
```

**Probably more for the discussion**
It is well known that following MSY policies can lead to over-fishing.
Our results reproduce this behavior and, furthermore, warn that using a TAC policy instead might not be enough to gain sustainability.
Figure \ref{rew_len_fig}-A summarizes episode length distribution for MSY, TAC and DRL.
While the DRL-derived policy does not lead to extinctions, both MSY and TAC do so, and thus their episode mean length is lower than the maximal episode length (in our case set at 200 years).
This difference is reflected on the profits earned by the agent, as shown in Fig. TBD-B.




## Acknowledgements

The title of this piece references a mathematical biology workshop at NIMBioS
organized by Paul Armsworth, Alan Hastings, Megan Donahue, and Carl Towes in
2011 which first sought to emphasize 'pretty darn good' control solutions to
more realistic problems over optimal control to idealized ones. 
This material is based upon work supported by the National Science Foundation under Grant No. DBI-1942280.

# References

