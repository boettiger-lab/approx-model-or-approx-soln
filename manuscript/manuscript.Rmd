---
title: "Pretty darn good control: when are approximate solutions better than approximate models"
titlerunning: Pretty darn good control
thanks: | 
    This material is based upon work supported by the National Science
    Foundation under Grant No. DBI-1942280. 

authors: 
- name: Felipe Montealegre-Mora
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Marcus Laperolerie
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Melissa Chapman
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Abigail G. Keller
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Carl Boettiger
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
  email: cboettig@berkeley.edu 


keywords:
- Optimal Control
- Reinforcement Learning
- Uncertainty
- Decision Theory

#PACS: 
#- PAC1
#- superPAC
    
MSC:
- MSC code 1
- MSC code 2

abstract: |
  Write something here
  
bibliography: bibliography.bib
#biblio-style: spphys

header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{lineno}
  - \usepackage{setspace}
  - \usepackage{bookmark}
  - \linenumbers
  - \doublespacing

# bibstyle options spbasic(default), spphys, spmpsci
# output: rticles::springer_article

output: 
  rticles::arxiv_article:
    keep_tex: true

editor_options: 
  markdown: 
    wrap: sentence

---


<!--
 Special Issue Description:

This special issue highlights the development of novel data-driven methods, including statistics, machine learning, parameter estimation, and uncertainty quantification, and combinations thereof, towards modeling biological systems. These newly developed methods will tackle challenges that are commonly encountered when modeling real-world experimental, field, pre-clinical, or clinical data. Examples of such challenges include high dimensionality, computational complexity, observation or process error, model bias, and intra- or inter-individual heterogeneity. Contributions to this special issue require validation of new methods with real-world data or simulated data sets that contain features of real-world data that exemplify an outlined modeling challenge. Papers should include a discussion justifying why the developed method is novel and not an application of previously developed methods, as well as how the method may be broadly applicable across different areas of biology, including medical, ecological, genetics, and epidemiological applications.
-->

# Introduction {#intro}

<!--
Existing methods for optimal control methods struggle to deal with the complexity commonly encountered in real-world systems, including dimensionality, process error, model bias and heterogeneity. 
Instead of tackling these complexities directly, researchers have typically sought to find exact optimal solutions to simplified models of the processes in question. 
When is the optimal solution to a very approximate, stylized model better than an approximate solution to a more accurate model? 
This question has largely gone unanswered owing to the difficulty of finding even approximate solutions in the case of complex models.  
Our approach draws on recent algorithmic and computational advances in deep reinforcement learning. 
These methods have hitherto focused on problems in games or robotic mechanics, which operate under precisely known rules. 
We demonstrate the ability for novel algorithms using deep neural networks to successfully approximate such solutions (the "policy function" or control rule) without knowing or ever attempting to infer a model for the process itself. 
This powerful new technique lets us finally begin to answer the question.
We show that in many but not all cases, the optimal policy for a carefully chosen over-simplified model can still out-perform these novel algorithms trained to find approximate solutions to simulations of a realistically complex system. 
This illustrates the promise of combining complex, realistic simulations with emerging machine-learning techniques to improve our understanding and management of ecological systems.
-->

Ecological management may be understood as an optimal control problem - a manager must determine a sequence of actions (a "policy") to optimize some desired objective(s) and constraints (e.g. conservation goals, natural resource extraction, economic and social constraints.)
Despite the complexity inherent in natural ecosystems, analytically tractable models have long played an essential role in their understanding and management.
...


<!-- Why fisheries-focused examples -->
Global fisheries management has long been a crucible for both the development of theory and best practices in actual management for many reasons. Fisheries are an essential natural resource that provide the primary source of protein for one in every four humans, and have faced widely documented declines due to overfishing [@Worm2006, @Costello2016]. 
Because of this importance, fisheries also represent some of the longest and best studied timeseries of population dynamics available to ecologists, as well as a correspondingly rich area for the development of theory and practice.
Methods developed in the study and management of fisheries are frequently applied to other areas of ecological management, from concepts such as maximum sustainable yield [], optimal control under observation uncertainty [@Memarzadeh2019], to adaptive management [@Walters], management strategy evaluation [@Punt], to the studies of conservation policy and international negotiations [@].

<!-- A sweeping summary of fisheries decision theory, don't bite me-->
To understand the importance of a potential application of novel data-driven methods to the real world, it is most instructive to have a clearer picture of how models are currently used to inform ecological management in fisheries.
In any issue as large and contentious as fisheries management, it may be far more difficult to provide a generalization  about how things actually work than to make sweeping recommendations about how it ought to.
All the same, it is our firm belief that any researcher seeking to do the latter should first attempt to understand the former, so we beg the forbearance of any specialists in the practice as we seek to provide a concise overview of fisheries management today.
Here, we focus specifically on practices in data rich, 'well-managed' fisheries, which, despite some issues, may be among the best examples of ecological management at continental scale that is largely driven by scientific processes, models and data. 

<!-- Fisheries population models are rich but decisions aren't, mostly constant mortality -->
While fisheries stock assessment models are quite rich, the choice of possible strategies remains motivated by intution from much simpler, analytically tractable one-dimensional models.
This fact is not always obvious at cursory inspection, and can be met with cynicism even among professional fisheries managers -- after all, typical stock assessment models have over 100 parameters (e.g. see the stock assessments referenced from in @RAMLegacy) and best practices such as management strategy evaluation (MSE) evaluate and select potential policies by comparing across large and diverse simulations [e.g. @Punt2016].
However, the space of policies considered under such approaches is necessarily far smaller than the set of all possible action sequences.
Instead, fisheries management approaches such as MSE rely on theory based on more tractable models to restrict the search space of possible policies.  
Specifically, most fisheries policies are framed in terms of a *constant mortality* policy, (sometimes a simple piece-wise linear function, i.e. a single linear segment, is introduced as a precaution a very low stock sizes), where fishing 'mortality' is defined as the ratio of harvest (catch) to stock size, $F = H_t/x_t$.

The intuition for a constant mortality strategy is simple and well established 

Constant mortality policies have a strong basis in theory as well as practical justifications.


<!-- In contrast to constant mortality, the possible decision space is huge-->
The management of any ecological system can be framed in terms of a Sequential Decision Process - 
the manager observes the ecosystem state ($x_t$) and decides on action ($a_t$) to take (including potentially no action), potentially receiving any costs or benefits associated with that action and and state (reward $r_t$  $r_t = U(x_t,a_t)$),
while the system evolves into a new state in response $x_{t+1} = f(x_t, a_t)$. When the probability of the future state can be determined by the current state and action, we say the system is Markovian and the process a Markov Decision Process (MDP).
As in a game of chess, the space of possible sequences of actions a manager might choose over any realization of the Markov process is enormous.
When the action and state space are discrete and sufficiently small, computational approaches such as dynamic programming may be used to guarantee an exhaustive search for a single optimal policy.
Dynamic programming scales very poorly to larger state and action spaces.

Deep reinforcement learning has opened up increasingly compelling alternative 

Effort $Q = E X_t$  

Fisheries policies are typically implemented in terms of *catch quotas*, i.e. a limit to the number of metric tonnes of fish that can be caught during a given season. 


and/or *effort controls*, restrictions on the fishing effort -- in numbers of boats, time of year, or days at sea




# Figures brainstorm

Figure 0: contrast 1D vs 3D strategies (see commented out diagram).

<!-- Reality __
  |        \
  v         \
1-D Model   More expressive model
  |                  |
  v                  v
Optimal        Suboptimal, but
solution   well-performing solution -->

Figure 1: 1-D and 3-D model conceptual figure. something about the objective / decision
Figure 2: Stability / multistability in the 1D and 3D models.  state space + time views
Figure 3: The 1-D optimal management solution.  'constant escapement' intuition etc

RL figures:
- schematic of RL (a-la previous marcus paper Fig 1)
- Neural network optimization figure

Results figures:
- timeseries example of management under the RL policy.  probably compare to managing under 1-D solution / rule-of-thumb methods
- state-space view of management doughnut 
- visualization / encapsulation of the policy heatmaps, slices, policy vs position along ellipse 
- reward plot over time (comparing methods)

# Mathematical models of fisheries

In this section we introduce some models describing the population dynamics of fisheries.
In general, the class of models that appear in this context are *first order finite difference equations*.
For $n$ species, these models have the general form
\begin{align}
  \label{eq:general model}
  N_{t+1} - N_t = f(N_t) - h_t := \Delta N_t,
\end{align}
where $N_t = (X_t,\ Y_t,\ \dots) \in \mathbb{R}^{n}_+$ is a vector of populations, $f:\mathbb{R}^{n}\to\mathbb{R}^{n}$ is an arbitrary function, and $h_t$ is the harvest size at timestep $t$. 

## A single species model

Most commonly, fisheries use 1-dimensional models, $n=1$.
One such model appearing in a variety of ecological contexts is *logistic growth*, for which
$$
  f(X_t) = r X_t\big(1 - X_t / K \big) =: L(X_t;\ r, K).
$$
More relevant to our current work, is the following single-species model studied in \cite{may77},
\begin{align}
  \label{eq:may}
  \Delta X_t
  = L(X_t;\ r, K) - F(X_t, H; \beta,c) - h_t,
\end{align}
where,
\begin{align*}
  F(X,H;\ \beta,c) := \frac{\beta H X^2}{c^2 + X^2}.
\end{align*}
The model has five parameters: the growth rate $r$ and carrying capacity $K$ for $X$, a constant population $H$ of a species which preys on $X$, the maximal rate of predation $\beta$, and the predation half-maximum biomass $c$.

Eq. \eqref{eq:may} is an interesting study case of a *tipping point* (see Fig. \ref{tbd}).
Holding the value of $\beta$ fixed, for intermediate values of $H$ there exist two stable fixed points for the state $V_1$ of the system, these two attractors separated by an unstable fixed point.
At a certain threshold value of $H$, however, the top stable fixed point collides with the unstable fixed point and both are annihilated.
For this value of $H$, and for higher values, only the lower fixed point remains.

This structure implies two things.
First, that a drift in $H$ could lead to catastrophic consequences, with the population $V_1$ plummeting to the lower fixed stable point.
Second, that if the evolution of $V_1$ is *stochastic*, then, even at values of $H$ below the threshold point, the system runs a sizeable danger of tipping over towards the lower stable point.

## A three species model

Consider the following three species *generalization* of eq. \eqref{eq:may}:
\begin{align}
\label{eq:3d model}
\begin{split}
  \Delta X_t &= L(X_t;\ r_X, K_X) - F(X_t, Z_t;\ \beta, c) - c_{XY}X_tY_t - h_t,\\
  \Delta Y_t &= L(Y_t;\ r_Y, K_Y) - D F(Y_t, Z_t;\ \beta, c) - c_{XY}X_tY_t,\\
  \Delta Z_t &= \left(f(X_t + DY_t\right) - d_Z)Z_t.
\end{split}
\end{align}
The model is conceptualized in Fig. \ref{fig:tbd}.
It contains three populations, the state at time $t$ is $(X_t, Y_t, Z_t)$. 
Species $Z$ preys on both $X$ and $Y$, while the latter two compete for resources.
There are ten parameters in this model: 
The growth rate and carrying capacity, $r_X$, $K_X$, $r_Y$ and $K_Y$, of $X$ and $Y$.
A parameter $c_{XY}$ mediating a Lotka-Volterra competition between $X$ and $Y$.
A maximum hunting intensity $\beta$ and half-maximum $c$ specifying how $Z$ forages on $X$ and $Y$.
A parameter $D$ regulating a relative preference of $Z$ to prey on $Y$.
Finally, a death rate $d_Z$ and a parameter $f$ related to the birth rate of $Z$.

This model generalizes eq. \eqref{eq:may} in the following sense:
If $f=d_Z=c_{XY}=0$, and we let $Z_{t=0}=H$, then eq. \eqref{eq:3d model} becomes
\begin{align*}
  \Delta X_t &= L(X_t;\ r_X, K_X) - F(X_t, H;\ \beta, c) - h_t,\\
  \Delta Y_t &= L(Y_t;\ r_Y, K_Y) - D F(Y_t, H;\ \beta, c),\\
  Z_t &= H.
\end{align*}
The equations for $X_t$ and $Y_t$ thus become decoupled, with $X_t$ evolving according to \eqref{eq:may}.

As can be expected, the dynamics of eq. \eqref{eq:3d model} are significantly more complex than those of eq. \eqref{eq:may}.
There are a variety of fixed points for the equations---around 7 or 8 for wide ranges of parameter values.
Fig. \ref{fig:tbd} provides a rough idea of the fixed point structure by plotting the bifurcation diagram obtained from varying $D$ and fixing the other parameters.

Throughout most of this paper we will fix the parameters of the model according to Table \ref{tab:tbd}.
At these values, the fixed points of the model are:
\begin{align*}
  TBD
\end{align*}
The classification of fixed points for multi-dimensional systems is generally more complicated than in one dimension.
A fixed point might be stable against perturbations in one direction, but unstable against perturbations along another.
While a detailed classification of these fixed points is beyond the scope of this work, we point out that the point $(TBD)$ is stable against perturbations in all directions.
We will thus use it as a starting point for our simulations.

## Stochastic dynamics

\textbf{Probably just merge on previous sections.}
We consider a stochastic contribution to the system's dynamics.
This way, eq. \eqref{eq:may} becomes
\begin{align}
  \label{eq:may stoch}
  \Delta X_t
  = L(X_t;\ r, K) - F(X_t, H; \beta,c) - h_t + \eta_t,
\end{align}
where $\{\eta_t\}_t$ are independent Gaussian variables with zero mean.
The variance $\sigma^2$ of $\eta_t$ is an additional parameter of the model.

Similarly, eq. \eqref{eq:3d model} becomes
\begin{align*}
  \label{eq:3d model stoch}
  \begin{split}
    \Delta X_t &= L(X_t;\ r_X, K_X) - F(X_t, Z_t;\ \beta, c) - c_{XY}X_tY_t - h_t + \eta_{X,t},\\
    \Delta Y_t &= L(Y_t;\ r_Y, K_Y) - D F(Y_t, Z_t;\ \beta, c) - c_{XY}X_tY_t + \eta_{Y,t},\\
    \Delta Z_t &= \left(f(X_t + DY_t\right) - d_Z)Z_t + \eta_{Z,t}.
\end{split}
\end{align*}





# Fishery management approaches

<!-- Introduce the 1D and 3D models, and have Figure 1 already here. -->

Here we review classical strategies for sustainable fishery management and we provide a birds-eye view of the alternative approach we propose.
A summary of the contrast between the two strategies is given in Fig. \ref{tbd}.

## Classical approaches to sustainable fisheries {#sec:classical}

There are several strategies that have been used to manage fisheries: *escapement*, *maximum sustainable yield (MSY)*, *total allowable catch (TAC)*, among others.
We collectively refere to these as *classical*, and will compare their performance to RL-based management strategies.
As shown in Fig. \ref{fig:tbd}, classical strategies have the common aspect of reducing the complex dynamics of the fishery ecosystem to a single equation governing the harvested population (say, $F$). 
A common example is using a logistic growth equation,
$$
  F_{t+1} - F_t = r F_t( 1 - F_t / K) - h_t =: L(F_t) - h_t,
$$
where the interaction between $F$ and its environment is summarized to two parameters, the growth rate $r$, and the carrying capacity $K$.
In the equation above, $h_t$ is the *harvest* at timestep $t$.
The goal is to choose the harvest policy $h:F_t\mapsto h_t$ such that long-term profits are maximized.

An advantage of one dimensional approaches is that the optimal policy is often known exactly, and, moreover, is intuitive.
For example, in the logistic equation pointed out above, the maximal sustainable yield of the system is attained at $F = F_{MSY} := K/2$.
The optimizer is an *escapement* policy:
$$
  h_t = \begin{cases}
    F_t - K/2, \qquad &\text{if, } F_t > K/2\\
    0, \qquad &\text{else.}
  \end{cases}
$$
This corresponds to keeping the system at its optimal growth rate as much as possible.

Escapement policies, or more generally *bang bang* policies, tend to be the optimal solution for these types of control problems.
A drawback of these solutions, in the fishery context, is the presence of several timesteps with zero harvest which can arise.
To mend this, certain suboptimal solutions have been constructed for fishery management.

One ubiquitous solution is simply called maximum sustainable yield (MSY).
It consists on letting $h(F) = rF/2$, so that
$$
  h(F_{MSY}) = L(F_{MSY}) = r K/4.
$$
That is, at the MSY biomass, the logistic growth of $F$ is cancelled exactly by the harvest.

The MSY rule fixes the drawback in the escapement policy by having $h(F)>0$ for all $F>0$.
It, however, has its own drawbacks.
It is particularly sensitive to misestimates of the parameter $r$, as we will discuss in Sec. \ref{sec:tbd}.
Due to this, similar but more conservative policies have been used.

*Total allowable catch (TAC)* is one such policy.
It consists on reducing the inclination of the line defined by $h(F)$ using a prefactor $\alpha$ in $h(F) = \alpha\times rF/2$. 
Plausible examples are $\alpha = 0.8$ or $0.9$.
Another common alternative is to have a prefactor $\alpha = \alpha(F)$ which decreases from one to zero as $F$ decreases below a threshold:
$$
  h(F) = \alpha(F)\times rF/2,
$$
where,
$$
  \alpha(F) = \begin{cases}
  1, \qquad &\text{if } F > F_{\text{thresh.}},\\
  F / F_{\text{thresh.}}, \qquad & \text{else}.
  \end{cases}
$$
We call this policy *variable total allowable catch (VTAC)*.

## Dangers: Noisy parameters, noisy observations and model inaccuracy


# Reinforcement learning {#sec:RL}

Reinforcement learning (RL) is, in a nutshell, a way of approaching *control problems* through machine learning.
An RL algorithm can be conceptually separated into two parts: an *agent*, and an *environment* which the agent can interact with.
That is, the agent may act on the environment and thus change its state, while the environment gives a *reward* to the agent in return (see Fig. \ref{tbd}).
The rewards encode the agent's goal.
The main part of an RL algorithm is then to progressively improve the agent's *policy*, in order to maximize the cumulative reward received. 
This is done by aggregating experience and learning from it.

<!-- The *environment* is commonly a computer simulation, although it sometimes can be a real world system.
The *agent*, on the other hand, is a computer program which interacts with the environment. -->

For our use case, the environment will be a computational model of the population dynamics of a fishery, with the environment state being a vector of all the fish populations, $S=(V_1, V_2, H)$.
At each time step, the agent harvests one of the populations, $V_1$. 
This changes the state as
\begin{align*}
  (V_1,\ V_2,\ H) \mapsto ((1-q)V_1,\ V_2,\ H),
\end{align*}
where $q$ is a fishing *quota* set by the agent. 
This secures a reward of $qV_1$.
Afterwards, the environment undergoes a timestep under its natural dynamics given by \eqref{eq:3d model}.

## Mathematical framework for RL

Mathematically, RL may be formulated using a discrete time *partially observable Markov decision process (POMDP)*.
This formalization is rather flexible and allows one, e.g., to account for situations where the agent may not fully observe the environment state, or where the only observations available to the agent are certain functions of the underlying state.
For the sake of clarity, we will present here only the class of POMDPs which are relevant to our work: *fully observable MDPs with a trivial emission function* (FMDPs for short).
An FMDP may be defined by the following data:

- $\mathcal{S}$: *state space*, the set of states of the environment,

- $\mathcal{A}$: *action space*, the set of actions which the agent may choose from,

- $T(s_{t+1}|s_t, a_t)$: *transition operator*, a conditional distribution which describes the dynamics of the system,

- $r(s_t, a_t)$: *reward function*, the reward obtained after performing action $a_t\in\mathcal{A}$ in state $s_t$,

- $d(s_0)$: *initial state distribution*, the initial state of the environment is sampled from this distribution,

- $\gamma\in[0,1]$: *discount factor*.

At a time $t$, the FMDP agent observes the full state $s_t$ of the environment and chooses an action based on this observation according to a *policy function* $\pi(a_t | s_t)$.
In return, it receives a discounted reward $\gamma^t r(a_t, s_t)$.
The discount factor helps regularize the agent, helping the optimization algorithm find solutions which pay off within a timescale of $t \sim \log(\gamma^{-1})^{-1}$.

With any fixed policy function, the agent will traverse a path 
$\tau=(s_0,\ a_0,\ s_1,\ a_1\ \dots,\ s_{t_{\text{fin.}}})$ 
sampled randomly from the distribution
\begin{align*}
  p_\pi(\tau) = 
  d(s_0) \prod_{t=0}^{ t_{\text{fin.}}-1 }
  \pi( a_t | s_t ) T( s_{t+1} | s_t, a_t ).
\end{align*}
Reinforcement learning seeks to optimize $\pi$ such that the expected rewards are maximal,
$$  
  \pi^* = \mathrm{argmax}\ \mathbb{E}_{\tau\sim p_\pi}[R(\tau)],
$$
where,
$$
  R(\tau) = \sum_{t=0}^{ t_{\text{fin.}}-1 } \gamma^t r(a_t, s_t),
$$
is the cumulative reward of path $\tau$.
The function $J(\pi):=\mathbb{E}_{\tau\sim p_\pi}[R(\tau)]$ is called the *value function*.

## Deep Reinforcement Learning

The policy function $\pi$ often lives in a high- or even infinite-dimensional space.
This makes it unfeasible to directly optimize $\pi$.
In practice, an alternative approach is used: $\pi$ is optimized over a much lower-dimensional parametrized family of functions.
Deep reinforcement learning uses this strategy, focusing on function families parametrized by neural networks. (See Fig. \ref{tbd}.)
<!-- The figure should kinda explain how the parametrization arises, and how gradient ascent is used. Maybe namedrop backpropagataion for the sake of usefulness to the reader? -->

Since our state and observation spaces are continuous, we will focus on deep reinforcement learning throughout this paper.
Specifically, we parametrize $\pi$ using a neural network with two hidden layers of 64 neurons.

We use the *proximal policy optimization (PPO)* algorithm to optimize $\pi$.
Within the RL literature there is a wealth of algorithms from which to choose from, each with its pros and cons.
Here we have focused on a single one of these for the sake of particularity---to draw a clear comparison between the RL-based and the classical fishery management approaches.
In practice, further improvements can be expected by a careful selection of the optimization algorithm.

## Model-free reinforcement learning

Within control theory, the usual setup is one where we use as much information from the model as possible in order to derive an optimal solution.
**(Wax poetic a bit about Bellmann eq. approaches here?)**

The classical sustainable fishery management approaches summarized in Sec. \ref{sec:classical} are model-based controls.
As we saw in that section, these controls may run into trouble in the case where there are inaccuracies in the model parameter estimates.

More generally, there are many situations in which the exact model of the system is not known or not tractable.
This is a standard situation in ecology: mathematical models capture the most prominent aspects of the ecosystem's dynamics, while ignoring or summarizing most of its complexity.
In this case, it is clear, model-based controls run a grave danger of mismanaging the system.

Reinforcement learning is, on the other hand, typically a model-free approach to control theory.\footnote{There are, however, approaches to perform model-based reinforcement learning. While we will not focus on these in this paper they are discussed in \cite{tbd1, tbd2}.}
While the model is certainly used to generate training data, it is not directly used by the optimization algorithm.
This provides more flexibility to use model-free RL in instances where the model of the system is not accurately known.
In fact, it has been shown to generally be preferable to use model-free RL in such instances.

**Mention CL here? Where solutions to other related problems in the curriculum are used to build solutions to the target problem.**

This context provides a motivation for this paper. 
Indeed, models for ecosystem dynamics are only ever approximate and incomplete descriptions of reality. 
This way, it is plausible that model-free RL controls outperform currently used model-based controls in ecological management problems.

  
  
# Results

We trained a DRL agent to obtain a candidate policy
\begin{align*}
  \pi: (X_t,Y_t,Z_t) \mapsto h_t
\end{align*}
for eq. \eqref{eq:3d model}.
The agent could fully and precisely observe the state of the system at each time step and decide on its harvest based on this information. 
The agent was trained using the PPO algorithm, using a neural network with two hidden layers of 64 neurons to express $\pi$.
Details of the training process may be found in App. \ref{app:tbd}.
Our code is available at TBD.

Rewards were defined to be a compound of two contributions.
The first is economical: if at time $t$ a harvest of $h_t$ is secured, then the agents total reward is increased by $h_t$.
The second regards sustainability: if an extinction happens at time step $t$, then a penalty of $-50/t$ is incurred by the agent.
The choice of the constant $50$ in this rule was rather arbitrary: we set it to be a fourth of the maximal episode length ($200$ time steps), so as to encourage the agent to find policies that do not lead to extinctions below the time scale of around 100 time steps.
\textbf{Too long of a "sorry" message?}

We compare the performance of this agent to the performance of MSY and of an 80% TAC. 

Our results suggest that DRL can find solutions that are more sustainable *and* more profitable than both MSY and TAC.
Indeed, DRL finds solutions in which, for certain periods of time, no harvest is performed on the system.
These periods effectively allow the system to, in a sense, "reset" itself.
They lead to a distinctive oscillating behavior of the system state over time, with periods of relative depletion followed by periods of growth. 
Figure \ref{fig:tbd} exemplifies this point by comparing the harvest effort and state times series obtained from these policies.

It is well known that following MSY policies can lead to over-fishing.
Our results reproduce this behavior and, furthermore, warn that using a TAC policy instead might not be enough to gain sustainability.
Figure TBD-A summarizes episode length distribution for MSY, TAC and DRL.
While the DRL-derived policy does not lead to extinctions, both MSY and TAC do so, and thus their episode mean length is lower than the maximal episode length (in our case set at 200 years).
This difference is reflected on the profits earned by the agent, as shown in Fig. TBD-B.


## Acknowledgements

The title of this piece references a mathematical biology workshop at NIMBioS
organized by Paul Armsworth, Alan Hastings, Megan Donahue, and Carl Towes in
2011 seeking to pivot control from 'optimal' to 'pretty darn good'.

# References

