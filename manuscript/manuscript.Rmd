---
title: "Pretty darn good control: when are approximate solutions better than approximate models"
titlerunning: Pretty darn good control
thanks: | 
    This material is based upon work supported by the National Science
    Foundation under Grant No. DBI-1942280. 

authors: 
- name: Felipe Montealegre-Mora
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Marcus Laperolerie
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Melissa Chapman
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Carl Boettiger
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
  email: cboettig@berkele.edu 


keywords:
- Optimal Control
- Reinforcement Learning
- Uncertainty
- Decision Theory

#PACS: 
#- PAC1
#- superPAC
    
MSC:
- MSC code 1
- MSC code 2

abstract: |
  The text of your abstract.  150 -- 250 words.

bibliography: bibliography.bib
biblio-style: spphys

# bibstyle options spbasic(default), spphys, spmpsci
output: rticles::springer_article

---

# Introduction {#intro}

<!-- Your text comes here. Separate text sections with \cite{Mislevy06Cog}. -->


# Figures brainstorm

Figure 1: 1-D and 3-D model conceptual figure. something about the objective / decision
Figure 2: Stability / multistability in the 1D and 3D models.  state space + time views

Figure 3: The 1-D optimal management solution.  'constant escapement' intuition etc

Results figures:
- timeseries example of management under the RL policy.  probably compare to managing under 1-D solution / rule-of-thumb methods
- state-space view of management doughnut 
- visualization / encapsulation of the policy heatmaps, slices, policy vs position along ellipse 
- reward plot over time (comparing methods)


# Reinforcement learning {#sec:DRL}

Reinforcement learning (RL) is a way of approaching *control problems* through machine learning.
An RL application can be conceptually separated into two parts: an *agent*, and an *environment*.
The *environment* is commonly a computer simulation.
The *agent*, on the other hand, is a computer program which interacts with the environment.
That is, the agent may act on the environment and change its state.
Such an action and its consequence, moreover, give the agent a *reward* which encodes our goal for the agent.
The main part of an RL algorithm is then to progressively improve the agent's *policy*, in order to maximize the cumulative reward received. 
This is done by aggregating experience and learning from it.

In our case, the environment will be a computational model of the population dynamics of a fishery.
The agent chooses how much to fish from a given species in the environment, thereby changing the populations in the environment's state.



\begin{align}
a^2+b^2=c^2
\end{align} -->

## Acknowledgements

The title of this piece references a mathematical biology workshop at NIMBioS
organized by Paul Armsworth, Alan Hastings, Megan Donahue, and Carl Towes in
2011 that first posed the question addressed here.

# References

