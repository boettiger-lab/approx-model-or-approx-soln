---
title: "Pretty darn good control: when are approximate solutions better than approximate models"
titlerunning: Pretty darn good control
thanks: | 
    This material is based upon work supported by the National Science
    Foundation under Grant No. DBI-1942280. 

authors: 
- name: Felipe Montealegre-Mora
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Marcus Laperolerie
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Melissa Chapman
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Carl Boettiger
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
  email: cboettig@berkele.edu 


keywords:
- Optimal Control
- Reinforcement Learning
- Uncertainty
- Decision Theory

#PACS: 
#- PAC1
#- superPAC
    
MSC:
- MSC code 1
- MSC code 2

abstract: |
  Existing methods for optimal control methods struggle to deal with the complexity commonly encountered in real-world systems, including dimensionality, process error, model bias and heterogeneity. 
  Instead of tackling these complexities directly, researchers have typically sought to find exact optimal solutions to simplified models of the processes in question. 
  When is the optimal solution to a very approximate, stylized model better than an approximate solution to a more accurate model? 
  This question has largely gone unanswered owing to the difficulty of finding even approximate solutions in the case of complex models.  
  Our approach draws on recent algorithmic and computational advances in deep reinforcement learning. 
  These methods have hitherto focused on problems in games or robotic mechanics, which operate under precisely known rules. 
  We demonstrate the ability for novel algorithms using deep neural networks to successfully approximate such solutions (the "policy function" or control rule) without knowing or ever attempting to infer a model for the process itself. 
  This powerful new technique lets us finally begin to answer the question.
  We show that in many but not all cases, the optimal policy for a carefully chosen over-simplified model can still out-perform these novel algorithms trained to find approximate solutions to simulations of a realistically complex system. 
  This illustrates the promise of combining complex, realistic simulations with emerging machine-learning techniques to improve our understanding and management of ecological systems.

bibliography: bibliography.bib
biblio-style: spphys

# bibstyle options spbasic(default), spphys, spmpsci
output: rticles::springer_article

---


<!--
 Special Issue Description:

This special issue highlights the development of novel data-driven methods, including statistics, machine learning, parameter estimation, and uncertainty quantification, and combinations thereof, towards modeling biological systems. These newly developed methods will tackle challenges that are commonly encountered when modeling real-world experimental, field, pre-clinical, or clinical data. Examples of such challenges include high dimensionality, computational complexity, observation or process error, model bias, and intra- or inter-individual heterogeneity. Contributions to this special issue require validation of new methods with real-world data or simulated data sets that contain features of real-world data that exemplify an outlined modeling challenge. Papers should include a discussion justifying why the developed method is novel and not an application of previously developed methods, as well as how the method may be broadly applicable across different areas of biology, including medical, ecological, genetics, and epidemiological applications.
-->

# Introduction {#intro}

Despite the complexity inherent in natural ecosystems, analytically tractable models have long played an essential role in their understanding and management.
Ecological management may be understood as an optimal control problem - a manager must determine a sequence of actions (a "policy") to optimize some desired objective or combination of objectives (conservation goals, natural resource extraction, economic and social constraints.)





# Figures brainstorm

Figure 1: 1-D and 3-D model conceptual figure. something about the objective / decision
Figure 2: Stability / multistability in the 1D and 3D models.  state space + time views

Figure 3: The 1-D optimal management solution.  'constant escapement' intuition etc

Results figures:
- timeseries example of management under the RL policy.  probably compare to managing under 1-D solution / rule-of-thumb methods
- state-space view of management doughnut 
- visualization / encapsulation of the policy heatmaps, slices, policy vs position along ellipse 
- reward plot over time (comparing methods)


# Reinforcement learning {#sec:DRL}

Reinforcement learning (RL) is a way of approaching *control problems* through machine learning.
An RL application can be conceptually separated into two parts: an *agent*, and an *environment*.
The *environment* is commonly a computer simulation.
The *agent*, on the other hand, is a computer program which interacts with the environment.
That is, the agent may act on the environment and change its state.
Such an action and its consequence, moreover, give the agent a *reward* which encodes our goal for the agent.
The main part of an RL algorithm is then to progressively improve the agent's *policy*, in order to maximize the cumulative reward received. 
This is done by aggregating experience and learning from it.

In our case, the environment will be a computational model of the population dynamics of a fishery.
The agent chooses how much to fish from a given species in the environment, thereby changing the populations in the environment's state.



\begin{align}
a^2+b^2=c^2
\end{align} -->

## Acknowledgements

The title of this piece references a mathematical biology workshop at NIMBioS
organized by Paul Armsworth, Alan Hastings, Megan Donahue, and Carl Towes in
2011 that first posed the question addressed here.

# References

