---
title: "Pretty darn good control: when are approximate solutions better than approximate models"
titlerunning: Pretty darn good control
thanks: | 
    This material is based upon work supported by the National Science
    Foundation under Grant No. DBI-1942280. 

authors: 
- name: Felipe Montealegre-Mora
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Marcus Laperolerie
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Melissa Chapman
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
- name: Carl Boettiger
  address: Department of Environmental Science, Policy, and Management, University of California, Berkeley
  email: cboettig@berkele.edu 


keywords:
- Optimal Control
- Reinforcement Learning
- Uncertainty
- Decision Theory

#PACS: 
#- PAC1
#- superPAC
    
MSC:
- MSC code 1
- MSC code 2

abstract: |
  The text of your abstract.  150 -- 250 words.

bibliography: bibliography.bib
biblio-style: spphys

# bibstyle options spbasic(default), spphys, spmpsci
output: rticles::springer_article

---

# Introduction {#intro}

<!-- Your text comes here. Separate text sections with \cite{Mislevy06Cog}. -->


# Figures brainstorm

Figure 1: 1-D and 3-D model conceptual figure. something about the objective / decision
Figure 2: Stability / multistability in the 1D and 3D models.  state space + time views

Figure 3: The 1-D optimal management solution.  'constant escapement' intuition etc

Results figures:
- timeseries example of management under the RL policy.  probably compare to managing under 1-D solution / rule-of-thumb methods
- state-space view of management doughnut 
- visualization / encapsulation of the policy heatmaps, slices, policy vs position along ellipse 
- reward plot over time (comparing methods)


# Reinforcement learning {#sec:DRL}

Reinforcement learning (RL) is a way of approaching *control problems* through machine learning.
An RL application can be conceptually separated into two parts: an *agent*, and an *environment*.
The agent may *act* on the environment.
This produces two effects: on the one hand, the state of the environment is changed, and on the other, the agent receives a *reward* (see Fig. ).
The reward depends on what action the agent chose, and on the initial and final states of the environment.
The reward encodes the goal which we desire the agent to acheive---in other words, it encodes how we wish the agent controls the environment.
The main part of any RL algorithm is then to progressively improve the agent's *policy*, the function the agent uses to choose which action to take given the state of the environment.
This is done by aggregating experience and learning from it.



\begin{align}
a^2+b^2=c^2
\end{align} -->

## Acknowledgements

The title of this piece references a mathematical biology workshop at NIMBioS
organized by Paul Armsworth, Alan Hastings, Megan Donahue, and Carl Towes in
2011 that first posed the question addressed here.

# References

